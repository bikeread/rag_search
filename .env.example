# LLM Settings
# Provider options: huggingface, ollama
LLM_PROVIDER=huggingface
# Model name - use smaller models for testing
LLM_MODEL=microsoft/phi-2
# Temperature for text generation (0.0-1.0)
LLM_TEMPERATURE=0.7
# Maximum tokens to generate
LLM_MAX_TOKENS=2048

# Ollama specific settings
OLLAMA_BASE_URL=http://localhost:11434
# Request timeout in seconds
OLLAMA_TIMEOUT=120

# Vector Database Settings
# Database type: faiss or milvus
VECTOR_DB_TYPE=faiss
# Path to store vector database files (for FAISS)
VECTOR_DB_PATH=data/vector_store.faiss
# Embedding model optimized for Chinese
EMBEDDING_MODEL=BAAI/bge-m3
# Device to use for embeddings (cpu or cuda)
EMBEDDING_DEVICE=cuda

# Milvus specific settings
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION=document_store
# Connection timeout in seconds
MILVUS_TIMEOUT=30
# Whether to use aliases for collections
MILVUS_USE_ALIASES=false


# RAG (Retrieval-Augmented Generation) Settings
# Chunk size for document splitting:
# - Large (1000-2000): Better for long documents, maintains context
# - Medium (500-1000): Good for general text
# - Small (200-500): Better for precise retrieval, good for code
CHUNK_SIZE=1024

# Overlap between chunks:
# - 10-20% of chunk size recommended
# - Larger overlap (20%) prevents information loss
# - Smaller overlap (10%) saves storage
CHUNK_OVERLAP=200

# Vector Search Settings
# Number of documents to retrieve
TOP_K_RESULTS=3

# Search multiplier for candidate selection
# Actual candidates = TOP_K_RESULTS * SEARCH_MULTIPLIER
# - Higher (3-5): Better recall, slower
# - Lower (1-2): Faster, might miss relevant docs
SEARCH_MULTIPLIER=2

# Similarity threshold for relevance
# - 0.7+: High relevance required
# - 0.3-0.7: Moderate relevance
# - <0.3: Might include noise
MIN_SIMILARITY_SCORE=0.5

# Vector Index Settings
# Use IVF index for large datasets
USE_IVF_INDEX=false
# Number of clusters for IVF index
IVF_NLIST=100
# Number of clusters to probe
IVF_NPROBE=10

# API Settings
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=false

# Monitoring Settings
PROMETHEUS_PORT=9090
LOG_LEVEL=INFO

# Document Processing Settings
DEFAULT_ENCODING=utf-8
DEFAULT_LANGUAGE=zh

# Unstructured Settings
UNSTRUCTURED_STRATEGY=fast  # fast or accurate
UNSTRUCTURED_OCR_LANGUAGES=eng,chi_sim
UNSTRUCTURED_ENABLE_OCR=true
UNSTRUCTURED_PDF_OCR_THRESHOLD=0.8
UNSTRUCTURED_IMAGE_MIN_SIZE=100
UNSTRUCTURED_HI_RES_MODEL=  # Optional: set to "chipper" for high-res docs

# 向量化配置
ENABLE_VECTORIZATION=true
VECTORIZATION_METHOD=bge-m3  # 可选：tfidf, word2vec, bert, bge-m3
VECTORIZATION_CACHE_DIR=./cache/vectorization

# TF-IDF配置
TFIDF_MAX_FEATURES=5000
TFIDF_USE_IDF=true

# Table Processing Settings
# 表格处理每个块的最大行数
TABLE_MAX_ROWS_PER_CHUNK=10
# 是否保留空单元格
TABLE_PRESERVE_EMPTY_CELLS=false
# 单元格分隔符
TABLE_CELL_SEPARATOR=" | "
# 键值对分隔符
TABLE_KEY_VALUE_SEPARATOR=": "
# 表头所在行（从0开始）
TABLE_HEADER_ROW=0 

# Word2Vec配置
WORD2VEC_VECTOR_SIZE=100
WORD2VEC_WINDOW=5
WORD2VEC_MIN_COUNT=1
WORD2VEC_WORKERS=4
WORD2VEC_PRETRAINED_PATH=

# BERT配置（如果未设置，将使用EMBEDDING_MODEL）
BERT_MODEL_NAME=bert-base-chinese
BERT_MAX_LENGTH=512

# BGE-M3配置（如果未设置，将使用EMBEDDING_MODEL）
BGE_MODEL_NAME=BAAI/bge-m3
BGE_MAX_LENGTH=512

# 查询增强配置
QUERY_ENHANCEMENT_ENABLED=true
QUERY_DECOMPOSITION_ENABLED=true
QUERY_EXPANSION_ENABLED=true
KEYWORD_EXTRACTION_ENABLED=true
MAX_ENHANCED_DOCS=10